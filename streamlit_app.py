# streamlit_app.py

import streamlit as st
import os
from prompts.system_prompt import load_system_prompt
from core.rag_engine import init_rag, ask_rag_question, ask_gpt_fallback
from core.meditation import read_meditation_text
from audio.tts import TTSClient
from audio.transcriber import transcribe_audio
from streamlit_webrtc import webrtc_streamer, WebRtcMode
from realtime_audio import AudioFrameRecorder

st.set_page_config(page_title="ì‚¬í‹° ì±—ë´‡", layout="centered")
st.title("ğŸ§˜ ì‚¬í‹° ë§ˆìŒì±™ê¹€ ì±—ë´‡")
st.markdown("ì‚¬í‹°ëŠ” ë‹¹ì‹ ì˜ ë‚´ë©´ì„ ìœ„í•œ ì¡°ìš©í•œ ì¹œêµ¬ì…ë‹ˆë‹¤.")

# ì´ˆê¸°í™”
rag_chain = init_rag("Mindfulness RAG")
system_prompt = load_system_prompt()
tts = TTSClient()

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": system_prompt}]

mode = st.radio("ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:", ["í…ìŠ¤íŠ¸ ì±„íŒ…", "ìŒì„± ì±„íŒ…", "ëª…ìƒë¬¸"])

# --- í…ìŠ¤íŠ¸ ì±„íŒ… ëª¨ë“œ ---
if mode == "í…ìŠ¤íŠ¸ ì±„íŒ…":
    st.subheader("ğŸ’¬ ì‚¬í‹°ì™€ ì±„íŒ…í•˜ì„¸ìš”")
    user_input = st.chat_input("ì‚¬í‹°ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”")

    for msg in st.session_state.messages:
        if msg["role"] != "system":
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    if user_input:
        st.session_state.messages.append({"role": "user", "content": user_input})

        with st.chat_message("assistant"):
            with st.spinner("ì‚¬í‹°ê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
                full_messages = st.session_state.messages
                answer = ask_gpt_fallback(full_messages)
                if not answer or "ëª¨ë¥´ê² " in answer:
                    st.info("GPT ë‹µë³€ì´ ë¶€ì¡±í•´ìš”. ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                    answer = ask_rag_question(rag_chain, user_input)

                st.markdown(answer)
                tts.speak(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})

# --- ìŒì„± ì±„íŒ… ëª¨ë“œ ---
elif mode == "ìŒì„± ì±„íŒ…":
    st.subheader("ğŸ¤ ìŒì„± ì…ë ¥ ë°©ì‹ ì„ íƒ")
    input_mode = st.radio("ì…ë ¥ ë°©ë²•:", ["íŒŒì¼ ì—…ë¡œë“œ", "ì‹¤ì‹œê°„ ë§ˆì´í¬ ë…¹ìŒ"])

    if input_mode == "íŒŒì¼ ì—…ë¡œë“œ":
        uploaded_file = st.file_uploader("ìŒì„± íŒŒì¼ ì—…ë¡œë“œ (WAV ë˜ëŠ” MP3)", type=["wav", "mp3"])
        if uploaded_file:
            with open("temp_audio.wav", "wb") as f:
                f.write(uploaded_file.read())
            try:
                user_input = transcribe_audio("temp_audio.wav")
                st.success(f"ğŸ—£ï¸ ì¸ì‹ëœ ì§ˆë¬¸: {user_input}")
                st.session_state.messages.append({"role": "user", "content": user_input})

                with st.chat_message("assistant"):
                    with st.spinner("ì‚¬í‹°ê°€ ì‘ë‹µ ì¤‘ì…ë‹ˆë‹¤..."):
                        full_messages = st.session_state.messages
                        answer = ask_gpt_fallback(full_messages)
                        if not answer or "ëª¨ë¥´ê² " in answer:
                            st.info("RAG ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
                            answer = ask_rag_question(rag_chain, user_input)
                        st.markdown(answer)
                        tts.speak(answer)
                        st.session_state.messages.append({"role": "assistant", "content": answer})

            except Exception as e:
                st.error(f"ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {e}")

    elif input_mode == "ì‹¤ì‹œê°„ ë§ˆì´í¬ ë…¹ìŒ":
        st.info("ğŸ™ï¸ ë§ˆì´í¬ì— ëŒ€ê³  ë§ì”€í•´ì£¼ì„¸ìš”. ì¤‘ì§€ ë²„íŠ¼ ëˆ„ë¥´ë©´ ì²˜ë¦¬ë©ë‹ˆë‹¤.")
        recorder = AudioFrameRecorder()
        webrtc_ctx = webrtc_streamer(
            key="speech",
            mode=WebRtcMode.SENDONLY,
            audio_receiver_size=1024,
            audio_frame_callback=recorder,
        )

        if st.button("â¹ï¸ ë…¹ìŒ ì¤‘ì§€ ë° ì²˜ë¦¬"):
            filepath = recorder.save_wav()
            if filepath:
                st.audio(filepath, format="audio/wav")
                try:
                    user_input = transcribe_audio(filepath)
                    st.success(f"ğŸ—£ï¸ ì¸ì‹ëœ ì§ˆë¬¸: {user_input}")
                    st.session_state.messages.append({"role": "user", "content": user_input})

                    with st.chat_message("assistant"):
                        with st.spinner("ì‚¬í‹°ê°€ ì‘ë‹µ ì¤‘ì…ë‹ˆë‹¤..."):
                            full_messages = st.session_state.messages
                            answer = ask_gpt_fallback(full_messages)
                            if not answer or "ëª¨ë¥´ê² " in answer:
                                st.info("RAG ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
                                answer = ask_rag_question(rag_chain, user_input)
                            st.markdown(answer)
                            tts.speak(answer)
                            st.session_state.messages.append({"role": "assistant", "content": answer})
                except Exception as e:
                    st.error(f"âŒ Whisper ì˜¤ë¥˜: {e}")

# --- ëª…ìƒë¬¸ ë³´ê¸°/ì¬ìƒ ëª¨ë“œ ---
elif mode == "ëª…ìƒë¬¸":
    st.subheader("ğŸ§˜ ëª…ìƒë¬¸ ì„ íƒ ë° ë“£ê¸°")
    folder_path = "mindfulness meditation"
    files = [f for f in os.listdir(folder_path) if f.endswith(".txt")]
    if not files:
        st.warning("ëª…ìƒë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        options = ["ë¬´ì‘ìœ„ ëª…ìƒë¬¸"] + files
        selected = st.selectbox("ëª…ìƒë¬¸ ì„ íƒ:", options)

        if st.button("ëª…ìƒë¬¸ ë³´ê¸° ë° ì¬ìƒ"):
            filepath = os.path.join(folder_path, files[0] if selected == "ë¬´ì‘ìœ„ ëª…ìƒë¬¸" else selected)
            try:
                with open(filepath, "r", encoding="utf-8") as f:
                    content = f.read()
                    st.text_area("ğŸ“– ëª…ìƒë¬¸ ë‚´ìš©", content, height=300)
                    tts.speak(content, is_meditation=True)
            except Exception as e:
                st.error(f"ëª…ìƒë¬¸ ë¡œë”© ì‹¤íŒ¨: {e}")

